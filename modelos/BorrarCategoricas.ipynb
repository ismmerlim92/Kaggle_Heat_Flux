{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "import scipy.io\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>geometry</th>\n",
       "      <th>pressure [MPa]</th>\n",
       "      <th>mass_flux [kg/m2-s]</th>\n",
       "      <th>x_e_out [-]</th>\n",
       "      <th>D_e [mm]</th>\n",
       "      <th>D_h [mm]</th>\n",
       "      <th>length [mm]</th>\n",
       "      <th>chf_exp [MW/m2]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Thompson</td>\n",
       "      <td>tube</td>\n",
       "      <td>7.00</td>\n",
       "      <td>3770.0</td>\n",
       "      <td>0.1754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.8</td>\n",
       "      <td>432.0</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Thompson</td>\n",
       "      <td>tube</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6049.0</td>\n",
       "      <td>-0.0416</td>\n",
       "      <td>10.3</td>\n",
       "      <td>10.3</td>\n",
       "      <td>762.0</td>\n",
       "      <td>6.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Thompson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.79</td>\n",
       "      <td>2034.0</td>\n",
       "      <td>0.0335</td>\n",
       "      <td>7.7</td>\n",
       "      <td>7.7</td>\n",
       "      <td>457.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Beus</td>\n",
       "      <td>annulus</td>\n",
       "      <td>13.79</td>\n",
       "      <td>3679.0</td>\n",
       "      <td>-0.0279</td>\n",
       "      <td>5.6</td>\n",
       "      <td>15.2</td>\n",
       "      <td>2134.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tube</td>\n",
       "      <td>13.79</td>\n",
       "      <td>686.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.1</td>\n",
       "      <td>11.1</td>\n",
       "      <td>457.0</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31639</th>\n",
       "      <td>31639</td>\n",
       "      <td>Thompson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1736.0</td>\n",
       "      <td>0.0886</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.8</td>\n",
       "      <td>591.0</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31640</th>\n",
       "      <td>31640</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31641</th>\n",
       "      <td>31641</td>\n",
       "      <td>Thompson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.27</td>\n",
       "      <td>658.0</td>\n",
       "      <td>-0.1224</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31642</th>\n",
       "      <td>31642</td>\n",
       "      <td>Thompson</td>\n",
       "      <td>tube</td>\n",
       "      <td>6.89</td>\n",
       "      <td>3825.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.6</td>\n",
       "      <td>23.6</td>\n",
       "      <td>1972.0</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31643</th>\n",
       "      <td>31643</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tube</td>\n",
       "      <td>6.89</td>\n",
       "      <td>7568.0</td>\n",
       "      <td>0.0603</td>\n",
       "      <td>12.8</td>\n",
       "      <td>12.8</td>\n",
       "      <td>1930.0</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31644 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    author geometry  pressure [MPa]  mass_flux [kg/m2-s]  \\\n",
       "0          0  Thompson     tube            7.00               3770.0   \n",
       "1          1  Thompson     tube             NaN               6049.0   \n",
       "2          2  Thompson      NaN           13.79               2034.0   \n",
       "3          3      Beus  annulus           13.79               3679.0   \n",
       "4          4       NaN     tube           13.79                686.0   \n",
       "...      ...       ...      ...             ...                  ...   \n",
       "31639  31639  Thompson      NaN             NaN               1736.0   \n",
       "31640  31640       NaN      NaN           13.79                  NaN   \n",
       "31641  31641  Thompson      NaN           18.27                658.0   \n",
       "31642  31642  Thompson     tube            6.89               3825.0   \n",
       "31643  31643       NaN     tube            6.89               7568.0   \n",
       "\n",
       "       x_e_out [-]  D_e [mm]  D_h [mm]  length [mm]  chf_exp [MW/m2]  \n",
       "0           0.1754       NaN      10.8        432.0              3.6  \n",
       "1          -0.0416      10.3      10.3        762.0              6.2  \n",
       "2           0.0335       7.7       7.7        457.0              2.5  \n",
       "3          -0.0279       5.6      15.2       2134.0              3.0  \n",
       "4              NaN      11.1      11.1        457.0              2.8  \n",
       "...            ...       ...       ...          ...              ...  \n",
       "31639       0.0886       NaN       7.8        591.0              2.3  \n",
       "31640          NaN       4.7       4.7          NaN              3.9  \n",
       "31641      -0.1224       3.0       3.0        150.0              2.3  \n",
       "31642          NaN      23.6      23.6       1972.0              3.7  \n",
       "31643       0.0603      12.8      12.8       1930.0              3.3  \n",
       "\n",
       "[31644 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir un diccionario de mapeo para los valores conocidos de \"author\" y \"geometry\"\n",
    "mapeo_geometry = {\n",
    "    'Thompson': 'tube',\n",
    "    'Peskov': 'tube',\n",
    "    'Weatherhead': 'tube',\n",
    "    'Inasaka': 'tube',\n",
    "    'Williams': 'tube',\n",
    "    'Beus': 'annulus',\n",
    "    'Janssen': 'annulus',\n",
    "    'Mortimore': 'annulus',\n",
    "    'Richenderfer': 'plate',\n",
    "    'Kossolapov': 'plate'\n",
    "}\n",
    "\n",
    "# Rellenar los valores nulos en \"geometry\" utilizando el diccionario de mapeo\n",
    "df['geometry'] = df['geometry'].fillna(df['author'].map(mapeo_geometry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llenar los nulos en \"length [mm]\" con 10.0 cuando la columna \"geometry\" es \"plate\"\n",
    "df.loc[df['geometry'] == 'plate', 'length [mm]'] = df.loc[df['geometry'] == 'plate', 'length [mm]'].fillna(10.0)\n",
    "\n",
    "# Llenar los nulos en \"D_e [mm]\" con 15.0 cuando la columna \"geometry\" es \"plate\"\n",
    "df.loc[df['geometry'] == 'plate', 'D_e [mm]'] = df.loc[df['geometry'] == 'plate', 'D_e [mm]'].fillna(15.0)\n",
    "\n",
    "# Llenar los nulos en \"D_h [mm]\" con 120.0 cuando la columna \"geometry\" es \"plate\"\n",
    "df.loc[df['geometry'] == 'plate', 'D_h [mm]'] = df.loc[df['geometry'] == 'plate', 'D_h [mm]'].fillna(120.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar las filas con valores nulos en la columna 'x_e_out'\n",
    "df_nulos = df[df['x_e_out [-]'].isnull()]\n",
    "\n",
    "# Crear un nuevo DataFrame con las filas que contienen valores nulos en 'x_e_out'\n",
    "nuevos_datos = df_nulos.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['x_e_out [-]'])\n",
    "\n",
    "# Imprimir el DataFrame original sin las filas con valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['geometry'] = df['geometry'].fillna('tube')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir un diccionario de mapeo para los valores conocidos de \"author\" y \"geometry\"\n",
    "mapeo_geometry = {\n",
    "    'Thompson': 'tube',\n",
    "    'Peskov': 'tube',\n",
    "    'Weatherhead': 'tube',\n",
    "    'Inasaka': 'tube',\n",
    "    'Williams': 'tube',\n",
    "    'Beus': 'annulus',\n",
    "    'Janssen': 'annulus',\n",
    "    'Mortimore': 'annulus',\n",
    "    'Richenderfer': 'plate',\n",
    "    'Kossolapov': 'plate'\n",
    "}\n",
    "\n",
    "# Rellenar los valores nulos en \"geometry\" utilizando el diccionario de mapeo\n",
    "nuevos_datos['geometry'] = nuevos_datos['geometry'].fillna(nuevos_datos['author'].map(mapeo_geometry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevos_datos['geometry'] = nuevos_datos['geometry'].fillna('tube')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['geometry'] == 'tube', 'author'] = df.loc[df['geometry'] == 'tube', 'author'].fillna('Thompson')\n",
    "df.loc[df['geometry'] == 'plate', 'author'] = df.loc[df['geometry'] == 'plate', 'author'].fillna('Richenderfer')\n",
    "df.loc[df['geometry'] == 'annulus', 'author'] = df.loc[df['geometry'] == 'annulus', 'author'].fillna('Beus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevos_datos.loc[nuevos_datos['geometry'] == 'tube', 'author'] = nuevos_datos.loc[nuevos_datos['geometry'] == 'tube', 'author'].fillna('Thompson')\n",
    "nuevos_datos.loc[nuevos_datos['geometry'] == 'plate', 'author'] = nuevos_datos.loc[nuevos_datos['geometry'] == 'plate', 'author'].fillna('Richenderfer')\n",
    "nuevos_datos.loc[nuevos_datos['geometry'] == 'annulus', 'author'] = nuevos_datos.loc[nuevos_datos['geometry'] == 'annulus', 'author'].fillna('Beus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar codificación one-hot a la columna \"author\"\n",
    "df = pd.get_dummies(df, columns=['author'], prefix=['author'])\n",
    "\n",
    "# Aplicar codificación one-hot a la columna \"geometry\"\n",
    "df = pd.get_dummies(df, columns=['geometry'], prefix=['geometry'])\n",
    "\n",
    "# Aplicar codificación one-hot a la columna \"author\"\n",
    "nuevos_datos = pd.get_dummies(nuevos_datos, columns=['author'], prefix=['author'])\n",
    "\n",
    "# Aplicar codificación one-hot a la columna \"geometry\"\n",
    "nuevos_datos = pd.get_dummies(nuevos_datos, columns=['geometry'], prefix=['geometry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir en X e y\n",
    "X = df.drop(['x_e_out [-]', 'id'], axis=1)  # Todas las columnas excepto 'x_e_out'\n",
    "y = df['x_e_out [-]']  # Solo la columna 'x_e_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar las características (X) y la columna objetivo (y)\n",
    "X_nuevos_datos = nuevos_datos.drop(columns=[\"x_e_out [-]\"])\n",
    "y_nuevos_datos = nuevos_datos[\"x_e_out [-]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.rename(columns={'pressure [MPa]': 'pressure', 'mass_flux [kg/m2-s]': 'mass_flux', \n",
    "                    'D_e [mm]': 'D_e', 'D_h [mm]': 'D_h', 'length [mm]': 'length'\n",
    "                    , 'chf_exp [MW/m2]': 'chf_exp'}, inplace=True)\n",
    "\n",
    "y.name = 'x_e_out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nuevos_datos.rename(columns={'pressure [MPa]': 'pressure', 'mass_flux [kg/m2-s]': 'mass_flux', \n",
    "                    'D_e [mm]': 'D_e', 'D_h [mm]': 'D_h', 'length [mm]': 'length'\n",
    "                    , 'chf_exp [MW/m2]': 'chf_exp'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_additional = pd.read_csv('data/Data_CHF_Zhao_2020_ATE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar codificación one-hot a la columna \"author\"\n",
    "df_additional = pd.get_dummies(df_additional, columns=['author'], prefix=['author'])\n",
    "\n",
    "# Aplicar codificación one-hot a la columna \"geometry\"\n",
    "df_additional = pd.get_dummies(df_additional, columns=['geometry'], prefix=['geometry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir en X e y\n",
    "X_additional = df_additional.drop(['x_e_out [-]', 'id'], axis=1)  # Todas las columnas excepto 'x_e_out'\n",
    "Y_additional = df_additional['x_e_out [-]']  # Solo la columna 'x_e_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_additional.rename(columns={'pressure [MPa]': 'pressure', 'mass_flux [kg/m2-s]': 'mass_flux', \n",
    "                    'D_e [mm]': 'D_e', 'D_h [mm]': 'D_h', 'length [mm]': 'length'\n",
    "                    , 'chf_exp [MW/m2]': 'chf_exp'}, inplace=True)\n",
    "\n",
    "Y_additional.name = 'x_e_out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 4: Concatenar X_additional y y_additional a X_train y y_train\n",
    "X_train = np.concatenate((X_train, X_additional), axis=0)\n",
    "y_train = np.concatenate((y_train, Y_additional), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but KNNImputer was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Paso 5: Crear el imputador KNN\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Paso 6: Imputar X_train\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "\n",
    "# Paso 7: Imputar X_test utilizando la misma imputación aprendida de X_train\n",
    "X_test = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Índices de columnas a eliminar\n",
    "column_indices_to_drop = [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
    "\n",
    "# Eliminar columnas de X_train\n",
    "X_train = np.delete(X_train, column_indices_to_drop, axis=1)\n",
    "\n",
    "# Eliminar columnas de X_test\n",
    "X_test = np.delete(X_test, column_indices_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nuevos_datos = X_nuevos_datos.drop(columns=[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Int64Index: 10415 entries, 4 to 31642\n",
      "Series name: x_e_out [-]\n",
      "Non-Null Count  Dtype  \n",
      "--------------  -----  \n",
      "0 non-null      float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 162.7 KB\n"
     ]
    }
   ],
   "source": [
    "y_nuevos_datos.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but KNNImputer was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# #Separar las características (X) y la columna objetivo (y)\n",
    "# X_nuevos_datos = X_nuevos_datos.drop(columns=[\"x_e_out\"])\n",
    "# y_nuevos_datos = nuevos_datos[\"x_e_out\"]\n",
    "\n",
    "# Imputar los valores nulos en las características (X)\n",
    "\n",
    "X_nuevos_datos = imputer.transform(X_nuevos_datos)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convertir el resultado a DataFrame\n",
    "#X_nuevos_datos = pd.DataFrame(X_nuevos_datos, columns=X_nuevos_datos.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.07405480054626608\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "model = XGBRegressor(colsample_bytree=0.5, learning_rate=0.01, max_depth=5, n_estimators=1000, subsample=0.6)\n",
    "\n",
    "# Paso 6: Crear el pipeline\n",
    "xg_pipeline = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Paso 7: Ajustar el pipeline a los datos de entrenamiento\n",
    "xg_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Paso 8: Realizar predicciones en los datos de prueba\n",
    "y_pred = xg_pipeline.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "# Calcular el error cuadrático medio (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calcular el RMSE\n",
    "rmse = math.sqrt(mse)\n",
    "\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separar las características (X) y la columna objetivo (y)\n",
    "X_nuevos_datos = nuevos_datos.drop(columns=[\"x_e_out [-]\"])\n",
    "y_nuevos_datos = nuevos_datos[\"x_e_out [-]\"]\n",
    "\n",
    "# Imputar los valores nulos en las características (X)\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "X_nuevos_datos_imputed = imputer.fit_transform(X_nuevos_datos)\n",
    "\n",
    "# Convertir el resultado a DataFrame\n",
    "X_nuevos_datos_imputed = pd.DataFrame(X_nuevos_datos_imputed, columns=X_nuevos_datos.columns)\n",
    "\n",
    "# # Crear el objeto de regresión lineal\n",
    "# regression_model = LinearRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nuevos_datos_imputed = X_nuevos_datos_imputed.drop(columns=[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 19 features, but StandardScaler is expecting 6 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Realizar predicciones en la columna objetivo (y) para los datos imputados\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y_pred1 \u001b[39m=\u001b[39m xg_pipeline\u001b[39m.\u001b[39;49mpredict(X_nuevos_datos_imputed)\n\u001b[0;32m      4\u001b[0m \u001b[39m# Combinar las predicciones con los datos originales\u001b[39;00m\n\u001b[0;32m      5\u001b[0m nuevos_datos[\u001b[39m\"\u001b[39m\u001b[39mx_e_out [-]\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m y_pred1\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:457\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    455\u001b[0m Xt \u001b[39m=\u001b[39m X\n\u001b[0;32m    456\u001b[0m \u001b[39mfor\u001b[39;00m _, name, transform \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter(with_final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 457\u001b[0m     Xt \u001b[39m=\u001b[39m transform\u001b[39m.\u001b[39;49mtransform(Xt)\n\u001b[0;32m    458\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mpredict(Xt, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpredict_params)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:975\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m    972\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    974\u001b[0m copy \u001b[39m=\u001b[39m copy \u001b[39mif\u001b[39;00m copy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy\n\u001b[1;32m--> 975\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    976\u001b[0m     X,\n\u001b[0;32m    977\u001b[0m     reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    978\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    979\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    980\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[0;32m    981\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    982\u001b[0m )\n\u001b[0;32m    984\u001b[0m \u001b[39mif\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(X):\n\u001b[0;32m    985\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\base.py:600\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    597\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 600\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[0;32m    602\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 400\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 19 features, but StandardScaler is expecting 6 features as input."
     ]
    }
   ],
   "source": [
    "\n",
    "# Realizar predicciones en la columna objetivo (y) para los datos imputados\n",
    "y_pred1 = xg_pipeline.predict(X_nuevos_datos_imputed)\n",
    "\n",
    "# Combinar las predicciones con los datos originales\n",
    "nuevos_datos[\"x_e_out [-]\"] = y_pred1\n",
    "\n",
    "# Los datos ahora tienen las predicciones en la columna objetivo (y)\n",
    "\n",
    "# Guardar los datos en un archivo para la competición de Kaggle\n",
    "nuevos_datos.to_csv(\"nuevos_datos_con_predicciones2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Leer el archivo CSV\n",
    "prueba1 = pd.read_csv(\"nuevos_datos_con_predicciones2.csv\")\n",
    "\n",
    "# Mantener solo las columnas \"id\" y \"x_e_out [mm]\"\n",
    "columnas_deseadas = [\"id\", \"x_e_out [-]\"]\n",
    "prueba1 = prueba1[columnas_deseadas]\n",
    "\n",
    "# Guardar el DataFrame resultante en un nuevo archivo CSV\n",
    "prueba1.to_csv(\"nuevos_datos_con_predicciones_filtrado10.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Random Forest): 0.0756613423620883\n",
      "RMSE (Gradient Boosting): 0.07479308216597548\n",
      "RMSE (AdaBoost): 0.10073675363448692\n",
      "RMSE (Extra Trees): 0.07853953066069258\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "# Paso 5: Definir las transformaciones y los modelos\n",
    "scaler = StandardScaler()\n",
    "rf_model = RandomForestRegressor(max_depth= 12, min_samples_leaf= 5, min_samples_split= 10, n_estimators= 100)\n",
    "gb_model = GradientBoostingRegressor(max_depth=5, n_estimators=100, learning_rate=0.1)\n",
    "ada_model = AdaBoostRegressor(n_estimators=100)\n",
    "et_model = ExtraTreesRegressor(min_samples_split=8, n_estimators= 400)\n",
    "\n",
    "# Paso 6: Crear los pipelines para Random Forest y Gradient Boosting\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('model', rf_model)\n",
    "])\n",
    "\n",
    "gb_pipeline = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('model', gb_model)\n",
    "])\n",
    "\n",
    "ada_pipeline = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('model', ada_model)\n",
    "])\n",
    "\n",
    "et_pipeline = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('model', et_model)\n",
    "])\n",
    "\n",
    "# Paso 7: Ajustar los pipelines a los datos de entrenamiento\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "gb_pipeline.fit(X_train, y_train)\n",
    "ada_pipeline.fit(X_train, y_train)\n",
    "et_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Paso 8: Realizar predicciones en los datos de prueba\n",
    "y_pred_rf = rf_pipeline.predict(X_test)\n",
    "y_pred_gb = gb_pipeline.predict(X_test)\n",
    "y_pred_ada = ada_pipeline.predict(X_test)\n",
    "y_pred_et = et_pipeline.predict(X_test)\n",
    "\n",
    "# Calcular el error cuadrático medio (MSE) y el RMSE para Random Forest\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "rmse_rf = math.sqrt(mse_rf)\n",
    "print(\"RMSE (Random Forest):\", rmse_rf)\n",
    "\n",
    "# Calcular el error cuadrático medio (MSE) y el RMSE para Gradient Boosting\n",
    "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
    "rmse_gb = math.sqrt(mse_gb)\n",
    "print(\"RMSE (Gradient Boosting):\", rmse_gb)\n",
    "\n",
    "# Calcular el error cuadrático medio (MSE) y el RMSE para AdaBoost\n",
    "mse_ada = mean_squared_error(y_test, y_pred_ada)\n",
    "rmse_ada = math.sqrt(mse_ada)\n",
    "print(\"RMSE (AdaBoost):\", rmse_ada)\n",
    "\n",
    "# Calcular el error cuadrático medio (MSE) y el RMSE para Extra Trees\n",
    "mse_et = mean_squared_error(y_test, y_pred_et)\n",
    "rmse_et = math.sqrt(mse_et)\n",
    "print(\"RMSE (Extra Trees):\", rmse_et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingRegressor(estimators=[(&#x27;XG&#x27;,\n",
       "                             Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                             (&#x27;model&#x27;,\n",
       "                                              XGBRegressor(base_score=None,\n",
       "                                                           booster=None,\n",
       "                                                           callbacks=None,\n",
       "                                                           colsample_bylevel=None,\n",
       "                                                           colsample_bynode=None,\n",
       "                                                           colsample_bytree=0.5,\n",
       "                                                           early_stopping_rounds=None,\n",
       "                                                           enable_categorical=False,\n",
       "                                                           eval_metric=None,\n",
       "                                                           feature_types=None,\n",
       "                                                           gamma=None,\n",
       "                                                           gpu_id=None,\n",
       "                                                           grow_policy=None,\n",
       "                                                           importance_t...\n",
       "                                                           max_cat_to_onehot=None,\n",
       "                                                           max_delta_step=None,\n",
       "                                                           max_depth=5,\n",
       "                                                           max_leaves=None,\n",
       "                                                           min_child_weight=None,\n",
       "                                                           missing=nan,\n",
       "                                                           monotone_constraints=None,\n",
       "                                                           n_estimators=1000,\n",
       "                                                           n_jobs=None,\n",
       "                                                           num_parallel_tree=None,\n",
       "                                                           predictor=None,\n",
       "                                                           random_state=None, ...))])),\n",
       "                            (&#x27;GraBoosst&#x27;,\n",
       "                             Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                             (&#x27;model&#x27;,\n",
       "                                              GradientBoostingRegressor(max_depth=5))]))],\n",
       "                weights=[0.8, 0.2])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingRegressor</label><div class=\"sk-toggleable__content\"><pre>VotingRegressor(estimators=[(&#x27;XG&#x27;,\n",
       "                             Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                             (&#x27;model&#x27;,\n",
       "                                              XGBRegressor(base_score=None,\n",
       "                                                           booster=None,\n",
       "                                                           callbacks=None,\n",
       "                                                           colsample_bylevel=None,\n",
       "                                                           colsample_bynode=None,\n",
       "                                                           colsample_bytree=0.5,\n",
       "                                                           early_stopping_rounds=None,\n",
       "                                                           enable_categorical=False,\n",
       "                                                           eval_metric=None,\n",
       "                                                           feature_types=None,\n",
       "                                                           gamma=None,\n",
       "                                                           gpu_id=None,\n",
       "                                                           grow_policy=None,\n",
       "                                                           importance_t...\n",
       "                                                           max_cat_to_onehot=None,\n",
       "                                                           max_delta_step=None,\n",
       "                                                           max_depth=5,\n",
       "                                                           max_leaves=None,\n",
       "                                                           min_child_weight=None,\n",
       "                                                           missing=nan,\n",
       "                                                           monotone_constraints=None,\n",
       "                                                           n_estimators=1000,\n",
       "                                                           n_jobs=None,\n",
       "                                                           num_parallel_tree=None,\n",
       "                                                           predictor=None,\n",
       "                                                           random_state=None, ...))])),\n",
       "                            (&#x27;GraBoosst&#x27;,\n",
       "                             Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                             (&#x27;model&#x27;,\n",
       "                                              GradientBoostingRegressor(max_depth=5))]))],\n",
       "                weights=[0.8, 0.2])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>XG</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.5, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=5, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=1000, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>GraBoosst</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(max_depth=5)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingRegressor(estimators=[('XG',\n",
       "                             Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                             ('model',\n",
       "                                              XGBRegressor(base_score=None,\n",
       "                                                           booster=None,\n",
       "                                                           callbacks=None,\n",
       "                                                           colsample_bylevel=None,\n",
       "                                                           colsample_bynode=None,\n",
       "                                                           colsample_bytree=0.5,\n",
       "                                                           early_stopping_rounds=None,\n",
       "                                                           enable_categorical=False,\n",
       "                                                           eval_metric=None,\n",
       "                                                           feature_types=None,\n",
       "                                                           gamma=None,\n",
       "                                                           gpu_id=None,\n",
       "                                                           grow_policy=None,\n",
       "                                                           importance_t...\n",
       "                                                           max_cat_to_onehot=None,\n",
       "                                                           max_delta_step=None,\n",
       "                                                           max_depth=5,\n",
       "                                                           max_leaves=None,\n",
       "                                                           min_child_weight=None,\n",
       "                                                           missing=nan,\n",
       "                                                           monotone_constraints=None,\n",
       "                                                           n_estimators=1000,\n",
       "                                                           n_jobs=None,\n",
       "                                                           num_parallel_tree=None,\n",
       "                                                           predictor=None,\n",
       "                                                           random_state=None, ...))])),\n",
       "                            ('GraBoosst',\n",
       "                             Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                             ('model',\n",
       "                                              GradientBoostingRegressor(max_depth=5))]))],\n",
       "                weights=[0.8, 0.2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "voting_reg1 = VotingRegressor([('XG', xg_pipeline), ('GraBoosst', gb_pipeline)], \n",
    "                              weights=[16/20, 4/20])\n",
    "voting_reg1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE):  0.0054899730897886405\n",
      "R-squared (R2) Score:  0.4595203212736271\n",
      "Root Mean Squared Error (RMSE):  0.07409435261737995\n"
     ]
    }
   ],
   "source": [
    "ensamble1=voting_reg1.predict(X_test)\n",
    "\n",
    "\n",
    "mse = mean_squared_error(y_test, ensamble1)\n",
    "r2 = r2_score(y_test, ensamble1)\n",
    "\n",
    "print(\"Mean Squared Error (MSE): \", mse)\n",
    "print(\"R-squared (R2) Score: \", r2)\n",
    "from math import sqrt\n",
    "\n",
    "#Calcular el RMSE\n",
    "\n",
    "rmse = sqrt(mse)\n",
    "print(\"Root Mean Squared Error (RMSE): \", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 19 features, but StandardScaler is expecting 6 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Realizar predicciones en la columna objetivo (y) para los datos imputados\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y_pred1 \u001b[39m=\u001b[39m voting_reg1\u001b[39m.\u001b[39;49mpredict(X_nuevos_datos_filtered)\n\u001b[0;32m      4\u001b[0m \u001b[39m# Combinar las predicciones con los datos originales\u001b[39;00m\n\u001b[0;32m      5\u001b[0m nuevos_datos[\u001b[39m\"\u001b[39m\u001b[39mx_e_out [-]\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m y_pred1\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:620\u001b[0m, in \u001b[0;36mVotingRegressor.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Predict regression target for X.\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \n\u001b[0;32m    606\u001b[0m \u001b[39mThe predicted regression target of an input sample is computed as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[39m    The predicted values.\u001b[39;00m\n\u001b[0;32m    618\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    619\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39maverage(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict(X), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, weights\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_weights_not_none)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:63\u001b[0m, in \u001b[0;36m_BaseVoting._predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_predict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m     62\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Collect results from clf.predict calls.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray([est\u001b[39m.\u001b[39;49mpredict(X) \u001b[39mfor\u001b[39;49;00m est \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimators_])\u001b[39m.\u001b[39mT\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:63\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_predict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m     62\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Collect results from clf.predict calls.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray([est\u001b[39m.\u001b[39;49mpredict(X) \u001b[39mfor\u001b[39;00m est \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_])\u001b[39m.\u001b[39mT\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:457\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    455\u001b[0m Xt \u001b[39m=\u001b[39m X\n\u001b[0;32m    456\u001b[0m \u001b[39mfor\u001b[39;00m _, name, transform \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter(with_final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 457\u001b[0m     Xt \u001b[39m=\u001b[39m transform\u001b[39m.\u001b[39;49mtransform(Xt)\n\u001b[0;32m    458\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mpredict(Xt, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpredict_params)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:975\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m    972\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    974\u001b[0m copy \u001b[39m=\u001b[39m copy \u001b[39mif\u001b[39;00m copy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy\n\u001b[1;32m--> 975\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    976\u001b[0m     X,\n\u001b[0;32m    977\u001b[0m     reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    978\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    979\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    980\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[0;32m    981\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    982\u001b[0m )\n\u001b[0;32m    984\u001b[0m \u001b[39mif\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(X):\n\u001b[0;32m    985\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\base.py:600\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    597\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 600\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[0;32m    602\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 400\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 19 features, but StandardScaler is expecting 6 features as input."
     ]
    }
   ],
   "source": [
    "\n",
    "# Realizar predicciones en la columna objetivo (y) para los datos imputados\n",
    "y_pred1 = voting_reg1.predict(X_nuevos_datos_filtered)\n",
    "\n",
    "# Combinar las predicciones con los datos originales\n",
    "nuevos_datos[\"x_e_out [-]\"] = y_pred1\n",
    "\n",
    "# Los datos ahora tienen las predicciones en la columna objetivo (y)\n",
    "\n",
    "# Guardar los datos en un archivo para la competición de Kaggle\n",
    "nuevos_datos.to_csv(\"nuevos_datos_con_predicciones2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Leer el archivo CSV\n",
    "prueba1 = pd.read_csv(\"nuevos_datos_con_predicciones2.csv\")\n",
    "\n",
    "# Mantener solo las columnas \"id\" y \"x_e_out [mm]\"\n",
    "columnas_deseadas = [\"id\", \"x_e_out [-]\"]\n",
    "prueba1 = prueba1[columnas_deseadas]\n",
    "\n",
    "# Guardar el DataFrame resultante en un nuevo archivo CSV\n",
    "prueba1.to_csv(\"nuevos_datos_con_predicciones_filtrado11.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros: {'model__max_depth': None, 'model__min_samples_split': 6, 'model__n_estimators': 300}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but KNNImputer was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.0810616503928035\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "# Paso 5: Definir las transformaciones y el modelo\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "scaler = StandardScaler()\n",
    "et_model = ExtraTreesRegressor()\n",
    "\n",
    "# Paso 6: Crear el pipeline\n",
    "et_pipeline = Pipeline([\n",
    "    ('imputer', imputer),\n",
    "    ('scaler', scaler),\n",
    "    ('model', et_model)\n",
    "])\n",
    "\n",
    "# Paso 7: Definir los parámetros de búsqueda en malla\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [None, 5, 10],\n",
    "    'model__min_samples_split': [2, 4, 6]\n",
    "}\n",
    "\n",
    "# Paso 8: Realizar la búsqueda en malla con validación cruzada\n",
    "grid_search = GridSearchCV(et_pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mejores hiperparámetros encontrados\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Mejores hiperparámetros:\", best_params)\n",
    "\n",
    "# Mejor modelo encontrado\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Paso 9: Realizar predicciones en los datos de prueba con el mejor modelo\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcular el error cuadrático medio (MSE) y el RMSE\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = math.sqrt(mse)\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "# Paso 5: Definir las transformaciones y el modelo\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "scaler = StandardScaler()\n",
    "ada_model = AdaBoostRegressor()\n",
    "\n",
    "# Paso 6: Crear el pipeline\n",
    "ada_pipeline = Pipeline([\n",
    "    ('imputer', imputer),\n",
    "    ('scaler', scaler),\n",
    "    ('model', ada_model)\n",
    "])\n",
    "\n",
    "# Paso 7: Definir los parámetros de búsqueda en malla\n",
    "param_grid = {\n",
    "    'model__n_estimators': [50, 100, 200],\n",
    "    'model__learning_rate': [0.01, 0.1, 1.0],\n",
    "}\n",
    "\n",
    "# Paso 8: Realizar la búsqueda en malla con validación cruzada\n",
    "grid_search = GridSearchCV(ada_pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mejores hiperparámetros encontrados\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Mejores hiperparámetros:\", best_params)\n",
    "\n",
    "# Mejor modelo encontrado\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Paso 9: Realizar predicciones en los datos de prueba con el mejor modelo\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcular el error cuadrático medio (MSE) y el RMSE\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = math.sqrt(mse)\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.790</td>\n",
       "      <td>686.0</td>\n",
       "      <td>11.1</td>\n",
       "      <td>11.1</td>\n",
       "      <td>457.0</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.000</td>\n",
       "      <td>750.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1650.0</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.414</td>\n",
       "      <td>2761.2</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>152.0</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.890</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>1930.0</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.510</td>\n",
       "      <td>1355.0</td>\n",
       "      <td>5.6</td>\n",
       "      <td>15.2</td>\n",
       "      <td>2134.0</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10410</th>\n",
       "      <td>11.030</td>\n",
       "      <td>3800.4</td>\n",
       "      <td>11.5</td>\n",
       "      <td>11.5</td>\n",
       "      <td>1270.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10411</th>\n",
       "      <td>1.010</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10412</th>\n",
       "      <td>13.790</td>\n",
       "      <td>688.0</td>\n",
       "      <td>11.1</td>\n",
       "      <td>11.1</td>\n",
       "      <td>457.0</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10413</th>\n",
       "      <td>13.790</td>\n",
       "      <td>3933.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.7</td>\n",
       "      <td>315.4</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10414</th>\n",
       "      <td>6.890</td>\n",
       "      <td>3825.0</td>\n",
       "      <td>23.6</td>\n",
       "      <td>23.6</td>\n",
       "      <td>1972.0</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10415 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0       1     2      3       4    5\n",
       "0      13.790   686.0  11.1   11.1   457.0  2.8\n",
       "1      18.000   750.0  10.0   10.0  1650.0  2.2\n",
       "2      12.414  2761.2   1.9    1.9   152.0  3.2\n",
       "3       6.890  7500.0  12.0   12.8  1930.0  4.8\n",
       "4      15.510  1355.0   5.6   15.2  2134.0  2.1\n",
       "...       ...     ...   ...    ...     ...  ...\n",
       "10410  11.030  3800.4  11.5   11.5  1270.0  2.0\n",
       "10411   1.010  2000.0  15.0  120.0    10.0  6.2\n",
       "10412  13.790   688.0  11.1   11.1   457.0  2.3\n",
       "10413  13.790  3933.2   4.7    4.7   315.4  3.9\n",
       "10414   6.890  3825.0  23.6   23.6  1972.0  3.7\n",
       "\n",
       "[10415 rows x 6 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Paso 1: Convertir X_nuevos_datos a DataFrame de Pandas\n",
    "X_nuevos_datos_df = pd.DataFrame(X_nuevos_datos)\n",
    "\n",
    "# Paso 2: Imputar valores nulos en X_nuevos_datos utilizando X_train\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "X_nuevos_datos_imputed = imputer.fit_transform(X_nuevos_datos_df, X_train)\n",
    "\n",
    "# # Paso 3: Escalar X_nuevos_datos_imputed\n",
    "# scaler = StandardScaler()\n",
    "# X_nuevos_datos_scaled = scaler.fit_transform(X_nuevos_datos_imputed)\n",
    "\n",
    "# Paso 4: Crear DataFrame de X_nuevos_datos_filtered sin las columnas específicas\n",
    "columns_to_drop = X_nuevos_datos_df.columns[6:19]\n",
    "X_nuevos_datos_filtered = pd.DataFrame(X_nuevos_datos_imputed, columns=X_nuevos_datos_df.columns)\n",
    "X_nuevos_datos_filtered = X_nuevos_datos_filtered.drop(columns=columns_to_drop, errors='ignore')\n",
    "X_nuevos_datos_filtered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pressure</th>\n",
       "      <th>mass_flux</th>\n",
       "      <th>D_e</th>\n",
       "      <th>D_h</th>\n",
       "      <th>length</th>\n",
       "      <th>chf_exp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.790</td>\n",
       "      <td>686.0</td>\n",
       "      <td>11.1</td>\n",
       "      <td>11.1</td>\n",
       "      <td>457.0</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.000</td>\n",
       "      <td>750.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1650.0</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.414</td>\n",
       "      <td>2761.2</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>152.0</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.890</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>1930.0</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.510</td>\n",
       "      <td>1355.0</td>\n",
       "      <td>5.6</td>\n",
       "      <td>15.2</td>\n",
       "      <td>2134.0</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10410</th>\n",
       "      <td>11.030</td>\n",
       "      <td>3800.4</td>\n",
       "      <td>11.5</td>\n",
       "      <td>11.5</td>\n",
       "      <td>1270.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10411</th>\n",
       "      <td>1.010</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10412</th>\n",
       "      <td>13.790</td>\n",
       "      <td>688.0</td>\n",
       "      <td>11.1</td>\n",
       "      <td>11.1</td>\n",
       "      <td>457.0</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10413</th>\n",
       "      <td>13.790</td>\n",
       "      <td>3933.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.7</td>\n",
       "      <td>315.4</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10414</th>\n",
       "      <td>6.890</td>\n",
       "      <td>3825.0</td>\n",
       "      <td>23.6</td>\n",
       "      <td>23.6</td>\n",
       "      <td>1972.0</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10415 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pressure  mass_flux   D_e    D_h  length  chf_exp\n",
       "0        13.790      686.0  11.1   11.1   457.0      2.8\n",
       "1        18.000      750.0  10.0   10.0  1650.0      2.2\n",
       "2        12.414     2761.2   1.9    1.9   152.0      3.2\n",
       "3         6.890     7500.0  12.0   12.8  1930.0      4.8\n",
       "4        15.510     1355.0   5.6   15.2  2134.0      2.1\n",
       "...         ...        ...   ...    ...     ...      ...\n",
       "10410    11.030     3800.4  11.5   11.5  1270.0      2.0\n",
       "10411     1.010     2000.0  15.0  120.0    10.0      6.2\n",
       "10412    13.790      688.0  11.1   11.1   457.0      2.3\n",
       "10413    13.790     3933.2   4.7    4.7   315.4      3.9\n",
       "10414     6.890     3825.0  23.6   23.6  1972.0      3.7\n",
       "\n",
       "[10415 rows x 6 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Obtener las columnas a mantener en X_nuevos_datos_filtered\n",
    "columns_to_keep = ['pressure', 'mass_flux', 'D_e', 'D_h', 'length', 'chf_exp']\n",
    "\n",
    "# Asignar los nombres de columna a X_nuevos_datos_filtered\n",
    "X_nuevos_datos_filtered.columns = columns_to_keep\n",
    "\n",
    "X_nuevos_datos_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Realizar predicciones en la columna objetivo (y) para los datos imputados\n",
    "y_pred1 = voting_reg1.predict(X_nuevos_datos_filtered)\n",
    "\n",
    "# Combinar las predicciones con los datos originales\n",
    "nuevos_datos[\"x_e_out [-]\"] = y_pred1\n",
    "\n",
    "# Los datos ahora tienen las predicciones en la columna objetivo (y)\n",
    "\n",
    "# Guardar los datos en un archivo para la competición de Kaggle\n",
    "nuevos_datos.to_csv(\"nuevos_datos_con_predicciones2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Leer el archivo CSV\n",
    "prueba1 = pd.read_csv(\"nuevos_datos_con_predicciones2.csv\")\n",
    "\n",
    "# Mantener solo las columnas \"id\" y \"x_e_out [mm]\"\n",
    "columnas_deseadas = [\"id\", \"x_e_out [-]\"]\n",
    "prueba1 = prueba1[columnas_deseadas]\n",
    "\n",
    "# Guardar el DataFrame resultante en un nuevo archivo CSV\n",
    "prueba1.to_csv(\"nuevos_datos_con_predicciones_filtrado12.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
